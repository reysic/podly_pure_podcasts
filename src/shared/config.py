from __future__ import annotations

from typing import Literal

from pydantic import BaseModel, Field, model_validator

from shared import defaults as DEFAULTS


class ProcessingConfig(BaseModel):
    num_segments_to_input_to_prompt: int
    max_overlap_segments: int = Field(
        default=DEFAULTS.PROCESSING_MAX_OVERLAP_SEGMENTS,
        ge=0,
        description="Maximum number of previously identified segments carried into the next prompt.",
    )

    @model_validator(mode="after")
    def validate_overlap_limits(self) -> ProcessingConfig:
        assert self.max_overlap_segments <= self.num_segments_to_input_to_prompt, (
            "max_overlap_segments must be <= num_segments_to_input_to_prompt"
        )
        return self


class OutputConfig(BaseModel):
    fade_ms: int
    min_ad_segement_separation_seconds: int
    min_ad_segment_length_seconds: int
    min_confidence: float

    @property
    def min_ad_segment_separation_seconds(self) -> int:
        """Backwards-compatible alias for the misspelled config field."""
        return self.min_ad_segement_separation_seconds

    @min_ad_segment_separation_seconds.setter
    def min_ad_segment_separation_seconds(self, value: int) -> None:
        self.min_ad_segement_separation_seconds = value


WhisperConfigTypes = Literal["remote", "test"]


class TestWhisperConfig(BaseModel):
    whisper_type: Literal["test"] = "test"


class RemoteWhisperConfig(BaseModel):
    whisper_type: Literal["remote"] = "remote"
    base_url: str = DEFAULTS.WHISPER_REMOTE_BASE_URL
    api_key: str
    language: str = DEFAULTS.WHISPER_REMOTE_LANGUAGE
    model: str = DEFAULTS.WHISPER_REMOTE_MODEL
    timeout_sec: int = DEFAULTS.WHISPER_REMOTE_TIMEOUT_SEC
    chunksize_mb: int = DEFAULTS.WHISPER_REMOTE_CHUNKSIZE_MB


class Config(BaseModel):
    llm_api_key: str | None = Field(default=None)
    llm_github_pat: str | None = Field(default=None)
    # Separate model for GitHub Copilot (uses PAT auth, different routing from llm_model)
    llm_github_model: str | None = Field(default=None)
    llm_model: str = Field(default=DEFAULTS.LLM_DEFAULT_MODEL)
    openai_base_url: str | None = None
    openai_max_tokens: int = DEFAULTS.OPENAI_DEFAULT_MAX_TOKENS
    openai_timeout: int = DEFAULTS.OPENAI_DEFAULT_TIMEOUT_SEC
    # Optional: Rate limiting controls
    llm_max_concurrent_calls: int = Field(
        default=DEFAULTS.LLM_DEFAULT_MAX_CONCURRENT_CALLS,
        description="Maximum concurrent LLM calls to prevent rate limiting",
    )
    llm_max_retry_attempts: int = Field(
        default=DEFAULTS.LLM_DEFAULT_MAX_RETRY_ATTEMPTS,
        description="Maximum retry attempts for failed LLM calls",
    )
    llm_max_input_tokens_per_call: int | None = Field(
        default=DEFAULTS.LLM_MAX_INPUT_TOKENS_PER_CALL,
        description="Maximum input tokens per LLM call to stay under API limits",
    )
    # Token-based rate limiting
    llm_enable_token_rate_limiting: bool = Field(
        default=DEFAULTS.LLM_ENABLE_TOKEN_RATE_LIMITING,
        description="Enable client-side token-based rate limiting",
    )
    llm_max_input_tokens_per_minute: int | None = Field(
        default=DEFAULTS.LLM_MAX_INPUT_TOKENS_PER_MINUTE,
        description="Override default tokens per minute limit for the model",
    )
    enable_boundary_refinement: bool = Field(
        default=DEFAULTS.ENABLE_BOUNDARY_REFINEMENT,
        description="Enable LLM-based ad boundary refinement for improved precision (consumes additional LLM tokens)",
    )
    enable_word_level_boundary_refinder: bool = Field(
        default=DEFAULTS.ENABLE_WORD_LEVEL_BOUNDARY_REFINDER,
        description="Enable word-level (heuristic-timed) ad boundary refinement",
    )
    developer_mode: bool = Field(
        default=False,
        description="Enable developer mode features like test feeds",
    )
    output: OutputConfig
    processing: ProcessingConfig
    server: str | None = Field(
        default=None,
        deprecated=True,
        description="deprecated in favor of request-aware URL generation",
    )
    background_update_interval_minute: int | None = (
        DEFAULTS.APP_BACKGROUND_UPDATE_INTERVAL_MINUTE
    )
    post_cleanup_retention_days: int | None = Field(
        default=DEFAULTS.APP_POST_CLEANUP_RETENTION_DAYS,
        description="Number of days to retain processed post data before cleanup. None disables cleanup.",
    )
    # removed job_timeout
    whisper: RemoteWhisperConfig | TestWhisperConfig | None = Field(
        default=None,
        discriminator="whisper_type",
    )
    remote_whisper: bool | None = Field(
        default=False,
        deprecated=True,
        description="deprecated in favor of [Remote|Local]WhisperConfig",
    )
    automatically_whitelist_new_episodes: bool = (
        DEFAULTS.APP_AUTOMATICALLY_WHITELIST_NEW_EPISODES
    )
    number_of_episodes_to_whitelist_from_archive_of_new_feed: int = (
        DEFAULTS.APP_NUM_EPISODES_TO_WHITELIST_FROM_ARCHIVE_OF_NEW_FEED
    )
    enable_public_landing_page: bool = DEFAULTS.APP_ENABLE_PUBLIC_LANDING_PAGE
    user_limit_total: int | None = DEFAULTS.APP_USER_LIMIT_TOTAL
    autoprocess_on_download: bool = DEFAULTS.APP_AUTOPROCESS_ON_DOWNLOAD

    @property
    def is_copilot_configured(self) -> bool:
        """True when both a GitHub PAT and a GitHub model are set.

        Copilot is the active LLM provider when this returns True.
        Having only a PAT without a model (or vice-versa) is treated as
        incomplete configuration and falls through to the OpenAI-compat path.
        """
        return bool(self.llm_github_pat) and bool(self.llm_github_model)

    @property
    def active_llm_model(self) -> str:
        """The model name that will actually be used for LLM calls.

        Returns the GitHub Copilot model when Copilot is fully configured,
        otherwise returns the standard llm_model (OpenAI-compat).
        """
        if self.is_copilot_configured and self.llm_github_model:
            return self.llm_github_model
        return self.llm_model

    def redacted(self) -> Config:
        return self.model_copy(
            update={
                "llm_api_key": "X" * 10,
                "llm_github_pat": "X" * 10,
            },
            deep=True,
        )

    @model_validator(mode="after")
    def validate_whisper_config(self) -> Config:
        new_style = self.whisper is not None

        if new_style:
            self.remote_whisper = None
            return self

        # if we have old style, change to the equivalent new style
        if self.remote_whisper:
            assert self.llm_api_key is not None, (
                "must supply api key to use remote whisper"
            )
            self.whisper = RemoteWhisperConfig(
                api_key=self.llm_api_key,
                base_url=self.openai_base_url or "https://api.openai.com/v1",
            )
        else:
            self.whisper = RemoteWhisperConfig(api_key="")

        self.remote_whisper = None

        return self

from __future__ import annotations

import logging
import os
from typing import Any

from flask import current_app

from app.db_commit import safe_commit
from app.extensions import db, scheduler
from app.models import (
    AppSettings,
    LLMSettings,
    OutputSettings,
    ProcessingSettings,
    WhisperSettings,
)
from app.runtime_config import config as runtime_config
from shared import defaults as DEFAULTS
from shared.config import Config as PydanticConfig
from shared.config import (
    RemoteWhisperConfig,
    TestWhisperConfig,
)

# pylint: disable=too-many-lines


logger = logging.getLogger("global_logger")


def _is_empty(value: Any) -> bool:
    return value is None or value == ""


def _parse_int(val: Any) -> int | None:
    try:
        return int(val) if val is not None else None
    except Exception:
        return None


def _parse_bool(val: Any) -> bool | None:
    if val is None:
        return None
    s = str(val).strip().lower()
    if s in {"1", "true", "yes", "on"}:
        return True
    if s in {"0", "false", "no", "off"}:
        return False
    return None


def _ensure_row(model: type, defaults: dict[str, Any]) -> Any:
    row = db.session.get(model, 1)
    if row is None:
        role = None
        try:
            role = current_app.config.get("PODLY_APP_ROLE")
        except Exception:  # pylint: disable=broad-except
            role = None

        # Web app should be read-only; only the writer process is allowed to create
        # missing settings rows.
        if role == "writer":
            row = model(id=1, **defaults)
            db.session.add(row)
            safe_commit(
                db.session,
                must_succeed=True,
                context="ensure_settings_row",
                logger_obj=logger,
            )
        else:
            logger.warning(
                "Settings row %s missing; returning defaults without persisting (role=%s)",
                getattr(model, "__name__", str(model)),
                role,
            )
            return model(id=1, **defaults)
    return row


def ensure_defaults() -> None:
    _ensure_row(
        LLMSettings,
        {
            "llm_model": DEFAULTS.LLM_DEFAULT_MODEL,
            "llm_github_pat": None,
            "llm_github_model": None,
            "openai_timeout": DEFAULTS.OPENAI_DEFAULT_TIMEOUT_SEC,
            "openai_max_tokens": DEFAULTS.OPENAI_DEFAULT_MAX_TOKENS,
            "llm_max_concurrent_calls": DEFAULTS.LLM_DEFAULT_MAX_CONCURRENT_CALLS,
            "llm_max_retry_attempts": DEFAULTS.LLM_DEFAULT_MAX_RETRY_ATTEMPTS,
            "llm_enable_token_rate_limiting": DEFAULTS.LLM_ENABLE_TOKEN_RATE_LIMITING,
            "enable_boundary_refinement": DEFAULTS.ENABLE_BOUNDARY_REFINEMENT,
            "enable_word_level_boundary_refinder": DEFAULTS.ENABLE_WORD_LEVEL_BOUNDARY_REFINDER,
        },
    )

    _ensure_row(
        WhisperSettings,
        {
            "whisper_type": DEFAULTS.WHISPER_DEFAULT_TYPE,
            "local_model": "base.en",
            "remote_model": DEFAULTS.WHISPER_REMOTE_MODEL,
            "remote_base_url": DEFAULTS.WHISPER_REMOTE_BASE_URL,
            "remote_language": DEFAULTS.WHISPER_REMOTE_LANGUAGE,
            "remote_timeout_sec": DEFAULTS.WHISPER_REMOTE_TIMEOUT_SEC,
            "remote_chunksize_mb": DEFAULTS.WHISPER_REMOTE_CHUNKSIZE_MB,
        },
    )

    _ensure_row(
        ProcessingSettings,
        {
            "num_segments_to_input_to_prompt": DEFAULTS.PROCESSING_NUM_SEGMENTS_TO_INPUT_TO_PROMPT,
        },
    )

    _ensure_row(
        OutputSettings,
        {
            "fade_ms": DEFAULTS.OUTPUT_FADE_MS,
            "min_ad_segement_separation_seconds": DEFAULTS.OUTPUT_MIN_AD_SEGMENT_SEPARATION_SECONDS,
            "min_ad_segment_length_seconds": DEFAULTS.OUTPUT_MIN_AD_SEGMENT_LENGTH_SECONDS,
            "min_confidence": DEFAULTS.OUTPUT_MIN_CONFIDENCE,
        },
    )

    _ensure_row(
        AppSettings,
        {
            "background_update_interval_minute": DEFAULTS.APP_BACKGROUND_UPDATE_INTERVAL_MINUTE,
            "automatically_whitelist_new_episodes": DEFAULTS.APP_AUTOMATICALLY_WHITELIST_NEW_EPISODES,
            "post_cleanup_retention_days": DEFAULTS.APP_POST_CLEANUP_RETENTION_DAYS,
            "number_of_episodes_to_whitelist_from_archive_of_new_feed": DEFAULTS.APP_NUM_EPISODES_TO_WHITELIST_FROM_ARCHIVE_OF_NEW_FEED,
            "enable_public_landing_page": DEFAULTS.APP_ENABLE_PUBLIC_LANDING_PAGE,
            "user_limit_total": DEFAULTS.APP_USER_LIMIT_TOTAL,
            "autoprocess_on_download": DEFAULTS.APP_AUTOPROCESS_ON_DOWNLOAD,
            "db_backup_enabled": DEFAULTS.APP_DB_BACKUP_ENABLED,
            "db_backup_interval_hours": DEFAULTS.APP_DB_BACKUP_INTERVAL_HOURS,
            "db_backup_retention_count": DEFAULTS.APP_DB_BACKUP_RETENTION_COUNT,
        },
    )


def _apply_llm_env_overlay(llm_data: dict[str, Any]) -> None:
    """Overlay environment variables onto the LLM config dict (no DB writes)."""
    env_llm_key = os.environ.get("LLM_API_KEY") or os.environ.get("OPENAI_API_KEY")
    if env_llm_key:
        llm_data["llm_api_key"] = env_llm_key

    env_github_pat = os.environ.get("GITHUB_PAT")
    if env_github_pat:
        llm_data["llm_github_pat"] = env_github_pat
        llm_data["github_pat"] = env_github_pat

    env_github_model = os.environ.get("GITHUB_MODEL")
    if env_github_model:
        llm_data["llm_github_model"] = env_github_model

    env_llm_model = os.environ.get("LLM_MODEL")
    if env_llm_model:
        llm_data["llm_model"] = env_llm_model

    env_openai_base_url = os.environ.get("OPENAI_BASE_URL")
    if env_openai_base_url:
        llm_data["openai_base_url"] = env_openai_base_url

    env_openai_timeout = _parse_int(os.environ.get("OPENAI_TIMEOUT"))
    if env_openai_timeout is not None:
        llm_data["openai_timeout"] = env_openai_timeout

    env_openai_max_tokens = _parse_int(os.environ.get("OPENAI_MAX_TOKENS"))
    if env_openai_max_tokens is not None:
        llm_data["openai_max_tokens"] = env_openai_max_tokens

    env_llm_max_concurrent = _parse_int(os.environ.get("LLM_MAX_CONCURRENT_CALLS"))
    if env_llm_max_concurrent is not None:
        llm_data["llm_max_concurrent_calls"] = env_llm_max_concurrent

    env_llm_max_retries = _parse_int(os.environ.get("LLM_MAX_RETRY_ATTEMPTS"))
    if env_llm_max_retries is not None:
        llm_data["llm_max_retry_attempts"] = env_llm_max_retries

    env_llm_enable_token_rl = _parse_bool(
        os.environ.get("LLM_ENABLE_TOKEN_RATE_LIMITING")
    )
    if env_llm_enable_token_rl is not None:
        llm_data["llm_enable_token_rate_limiting"] = bool(env_llm_enable_token_rl)

    env_llm_max_input_tokens_per_call = _parse_int(
        os.environ.get("LLM_MAX_INPUT_TOKENS_PER_CALL")
    )
    if env_llm_max_input_tokens_per_call is not None:
        llm_data["llm_max_input_tokens_per_call"] = env_llm_max_input_tokens_per_call

    env_llm_max_input_tokens_per_minute = _parse_int(
        os.environ.get("LLM_MAX_INPUT_TOKENS_PER_MINUTE")
    )
    if env_llm_max_input_tokens_per_minute is not None:
        llm_data["llm_max_input_tokens_per_minute"] = (
            env_llm_max_input_tokens_per_minute
        )


def _apply_whisper_env_overlay(whisper_data: dict[str, Any]) -> None:
    """Overlay environment variables onto the whisper config dict (no DB writes)."""
    env_whisper_key = (
        os.environ.get("WHISPER_API_KEY")
        or os.environ.get("WHISPER_REMOTE_API_KEY")
        or os.environ.get("OPENAI_API_KEY")
    )
    if env_whisper_key:
        whisper_data["api_key"] = env_whisper_key

    env_whisper_base = (
        os.environ.get("WHISPER_BASE_URL")
        or os.environ.get("WHISPER_REMOTE_BASE_URL")
        or os.environ.get("OPENAI_BASE_URL")
    )
    if env_whisper_base:
        whisper_data["base_url"] = env_whisper_base

    env_whisper_model = os.environ.get("WHISPER_MODEL") or os.environ.get(
        "WHISPER_REMOTE_MODEL"
    )
    if env_whisper_model:
        whisper_data["model"] = env_whisper_model

    env_whisper_timeout = _parse_int(
        os.environ.get("WHISPER_TIMEOUT_SEC")
        or os.environ.get("WHISPER_REMOTE_TIMEOUT_SEC")
    )
    if env_whisper_timeout is not None:
        whisper_data["timeout_sec"] = env_whisper_timeout

    env_whisper_chunksize = _parse_int(
        os.environ.get("WHISPER_CHUNKSIZE_MB")
        or os.environ.get("WHISPER_REMOTE_CHUNKSIZE_MB")
    )
    if env_whisper_chunksize is not None:
        whisper_data["chunksize_mb"] = env_whisper_chunksize


def read_combined() -> dict[str, Any]:
    """Read combined config from DB, then overlay any environment variable overrides.

    Environment variables always win over DB-stored values at read time.
    Nothing is written to the DB here — env values are ephemeral overlays.
    """
    ensure_defaults()

    llm = LLMSettings.query.get(1)
    whisper = WhisperSettings.query.get(1)
    processing = ProcessingSettings.query.get(1)
    output = OutputSettings.query.get(1)
    app_s = AppSettings.query.get(1)

    assert llm and whisper and processing and output and app_s

    whisper_payload: dict[str, Any] = {"whisper_type": whisper.whisper_type}
    if whisper.whisper_type == "remote":
        whisper_payload.update(
            {
                "model": whisper.remote_model,
                "api_key": whisper.remote_api_key,
                "base_url": whisper.remote_base_url,
                "language": whisper.remote_language,
                "timeout_sec": whisper.remote_timeout_sec,
                "chunksize_mb": whisper.remote_chunksize_mb,
            }
        )
    elif whisper.whisper_type == "test":
        whisper_payload.update({})

    llm_payload: dict[str, Any] = {
        "llm_api_key": llm.llm_api_key,
        "github_pat": llm.llm_github_pat,
        "llm_github_pat": llm.llm_github_pat,
        "llm_github_model": llm.llm_github_model,
        "llm_model": llm.llm_model,
        "openai_base_url": llm.openai_base_url,
        "openai_timeout": llm.openai_timeout,
        "openai_max_tokens": llm.openai_max_tokens,
        "llm_max_concurrent_calls": llm.llm_max_concurrent_calls,
        "llm_max_retry_attempts": llm.llm_max_retry_attempts,
        "llm_max_input_tokens_per_call": llm.llm_max_input_tokens_per_call,
        "llm_enable_token_rate_limiting": llm.llm_enable_token_rate_limiting,
        "llm_max_input_tokens_per_minute": llm.llm_max_input_tokens_per_minute,
        "enable_boundary_refinement": llm.enable_boundary_refinement,
        "enable_word_level_boundary_refinder": llm.enable_word_level_boundary_refinder,
    }

    # Overlay env vars — env always wins, but nothing is persisted
    _apply_llm_env_overlay(llm_payload)
    if whisper.whisper_type == "remote":
        _apply_whisper_env_overlay(whisper_payload)

    return {
        "llm": llm_payload,
        "whisper": whisper_payload,
        "processing": {
            "num_segments_to_input_to_prompt": processing.num_segments_to_input_to_prompt,
        },
        "output": {
            "fade_ms": output.fade_ms,
            "min_ad_segement_separation_seconds": output.min_ad_segement_separation_seconds,
            "min_ad_segment_length_seconds": output.min_ad_segment_length_seconds,
            "min_confidence": output.min_confidence,
        },
        "app": {
            "background_update_interval_minute": app_s.background_update_interval_minute,
            "automatically_whitelist_new_episodes": app_s.automatically_whitelist_new_episodes,
            "post_cleanup_retention_days": app_s.post_cleanup_retention_days,
            "number_of_episodes_to_whitelist_from_archive_of_new_feed": app_s.number_of_episodes_to_whitelist_from_archive_of_new_feed,
            "enable_public_landing_page": app_s.enable_public_landing_page,
            "user_limit_total": app_s.user_limit_total,
            "autoprocess_on_download": app_s.autoprocess_on_download,
            "db_backup_enabled": app_s.db_backup_enabled,
            "db_backup_interval_hours": app_s.db_backup_interval_hours,
            "db_backup_last_success_at": (
                app_s.db_backup_last_success_at.isoformat()
                if app_s.db_backup_last_success_at
                else None
            ),
            "db_backup_retention_count": app_s.db_backup_retention_count,
        },
    }


def _update_section_llm(data: dict[str, Any]) -> None:
    row = LLMSettings.query.get(1)
    assert row is not None
    for key in [
        "llm_api_key",
        "llm_github_pat",
        "llm_github_model",
        "llm_model",
        "openai_base_url",
        "openai_timeout",
        "openai_max_tokens",
        "llm_max_concurrent_calls",
        "llm_max_retry_attempts",
        "llm_max_input_tokens_per_call",
        "llm_enable_token_rate_limiting",
        "llm_max_input_tokens_per_minute",
        "enable_boundary_refinement",
        "enable_word_level_boundary_refinder",
    ]:
        if key in data:
            new_val = data[key]
            if key == "llm_api_key" and _is_empty(new_val):
                continue
            setattr(row, key, new_val)
    # Backwards/alternative key for UI: accept `github_pat` as well
    if "github_pat" in data:
        new_val = data["github_pat"]
        if not _is_empty(new_val):
            row.llm_github_pat = new_val
    safe_commit(
        db.session,
        must_succeed=True,
        context="update_llm_settings",
        logger_obj=logger,
    )


def _update_section_whisper(data: dict[str, Any]) -> None:
    row = WhisperSettings.query.get(1)
    assert row is not None
    if "whisper_type" in data and data["whisper_type"] in {
        "remote",
        "test",
    }:
        row.whisper_type = data["whisper_type"]
    if row.whisper_type == "remote":
        for key_map in [
            ("model", "remote_model"),
            ("api_key", "remote_api_key"),
            ("base_url", "remote_base_url"),
            ("language", "remote_language"),
            ("timeout_sec", "remote_timeout_sec"),
            ("chunksize_mb", "remote_chunksize_mb"),
        ]:
            src, dst = key_map
            if src in data:
                new_val = data[src]
                if src == "api_key" and _is_empty(new_val):
                    continue
                setattr(row, dst, new_val)
    else:
        # test type has no extra fields
        pass
    safe_commit(
        db.session,
        must_succeed=True,
        context="update_whisper_settings",
        logger_obj=logger,
    )


def _update_section_processing(data: dict[str, Any]) -> None:
    row = ProcessingSettings.query.get(1)
    assert row is not None
    for key in [
        "num_segments_to_input_to_prompt",
    ]:
        if key in data:
            setattr(row, key, data[key])
    safe_commit(
        db.session,
        must_succeed=True,
        context="update_processing_settings",
        logger_obj=logger,
    )


def _update_section_output(data: dict[str, Any]) -> None:
    row = OutputSettings.query.get(1)
    assert row is not None
    for key in [
        "fade_ms",
        "min_ad_segement_separation_seconds",
        "min_ad_segment_length_seconds",
        "min_confidence",
    ]:
        if key in data:
            setattr(row, key, data[key])
    safe_commit(
        db.session,
        must_succeed=True,
        context="update_output_settings",
        logger_obj=logger,
    )


def _update_section_app(
    data: dict[str, Any],
) -> tuple[int | None, int | None, bool | None, int | None]:
    row = AppSettings.query.get(1)
    assert row is not None
    old_interval: int | None = row.background_update_interval_minute
    old_retention: int | None = row.post_cleanup_retention_days
    old_backup_enabled: bool | None = row.db_backup_enabled
    old_backup_hours: int | None = row.db_backup_interval_hours
    for key in [
        "background_update_interval_minute",
        "automatically_whitelist_new_episodes",
        "post_cleanup_retention_days",
        "number_of_episodes_to_whitelist_from_archive_of_new_feed",
        "enable_public_landing_page",
        "user_limit_total",
        "autoprocess_on_download",
        "db_backup_enabled",
        "db_backup_interval_hours",
        "db_backup_retention_count",
    ]:
        if key in data:
            setattr(row, key, data[key])
    safe_commit(
        db.session,
        must_succeed=True,
        context="update_app_settings",
        logger_obj=logger,
    )
    return old_interval, old_retention, old_backup_enabled, old_backup_hours


def _maybe_reschedule_refresh_job(
    old_interval: int | None, new_interval: int | None
) -> None:
    if old_interval == new_interval:
        return

    job_id = "refresh_all_feeds"
    job = scheduler.get_job(job_id)

    if new_interval is None:
        if job:
            try:
                scheduler.remove_job(job_id)
            except Exception:
                pass
        return

    if not job:
        return

    # Avoid importing app.background here (it creates a cycle for pylint).
    # Use best-effort rescheduling on the underlying APScheduler instance.
    scheduler_obj = getattr(scheduler, "scheduler", scheduler)
    reschedule = getattr(scheduler_obj, "reschedule_job", None)
    if callable(reschedule):
        reschedule(job_id, trigger="interval", minutes=int(new_interval))


def _maybe_disable_cleanup_job(
    old_retention: int | None, new_retention: int | None
) -> None:
    if old_retention == new_retention:
        return

    job_id = "cleanup_processed_posts"
    job = scheduler.get_job(job_id)

    if new_retention is None or new_retention <= 0:
        if job:
            try:
                scheduler.remove_job(job_id)
            except Exception:
                pass


def _maybe_reschedule_db_backup_job(
    old_enabled: bool | None,
    old_hours: int | None,
    new_enabled: bool | None,
    new_hours: int | None,
) -> None:
    """Reschedule or remove the DB backup APScheduler job when settings change."""
    if old_enabled == new_enabled and old_hours == new_hours:
        return

    from app.background import (  # pylint: disable=import-outside-toplevel
        schedule_db_backup_job,
    )

    schedule_db_backup_job(
        hours=new_hours if new_hours is not None else 0,
        enabled=bool(new_enabled),
    )


def update_combined(payload: dict[str, Any]) -> dict[str, Any]:
    if "llm" in payload:
        _update_section_llm(payload["llm"] or {})
    if "whisper" in payload:
        _update_section_whisper(payload["whisper"] or {})
    if "processing" in payload:
        _update_section_processing(payload["processing"] or {})
    if "output" in payload:
        _update_section_output(payload["output"] or {})
    if "app" in payload:
        old_interval, old_retention, old_backup_enabled, old_backup_hours = (
            _update_section_app(payload["app"] or {})
        )

        app_s = AppSettings.query.get(1)
        if app_s:
            _maybe_reschedule_refresh_job(
                old_interval, app_s.background_update_interval_minute
            )
            _maybe_disable_cleanup_job(old_retention, app_s.post_cleanup_retention_days)
            _maybe_reschedule_db_backup_job(
                old_backup_enabled,
                old_backup_hours,
                app_s.db_backup_enabled,
                app_s.db_backup_interval_hours,
            )

    return read_combined()


def to_pydantic_config() -> PydanticConfig:
    data = read_combined()
    # Map whisper section to discriminated union config
    whisper_obj: RemoteWhisperConfig | TestWhisperConfig | None = None
    w = data["whisper"]
    wtype = w.get("whisper_type")
    if wtype == "remote":
        whisper_obj = RemoteWhisperConfig(
            model=w.get("model", "whisper-1"),
            # Allow boot without a remote API key so the UI can be used to set it
            api_key=w.get("api_key") or "",
            base_url=w.get("base_url", "https://api.openai.com/v1"),
            language=w.get("language", "en"),
            timeout_sec=w.get("timeout_sec", 600),
            chunksize_mb=w.get("chunksize_mb", 24),
        )
    elif wtype == "test":
        whisper_obj = TestWhisperConfig()

    return PydanticConfig(
        llm_api_key=data["llm"].get("llm_api_key"),
        llm_github_pat=data["llm"].get("llm_github_pat")
        or data["llm"].get("github_pat"),
        llm_github_model=data["llm"].get("llm_github_model"),
        llm_model=data["llm"].get("llm_model", DEFAULTS.LLM_DEFAULT_MODEL),
        openai_base_url=data["llm"].get("openai_base_url"),
        openai_max_tokens=int(
            data["llm"].get("openai_max_tokens", DEFAULTS.OPENAI_DEFAULT_MAX_TOKENS)
            or DEFAULTS.OPENAI_DEFAULT_MAX_TOKENS
        ),
        openai_timeout=int(
            data["llm"].get("openai_timeout", DEFAULTS.OPENAI_DEFAULT_TIMEOUT_SEC)
            or DEFAULTS.OPENAI_DEFAULT_TIMEOUT_SEC
        ),
        llm_max_concurrent_calls=int(
            data["llm"].get(
                "llm_max_concurrent_calls", DEFAULTS.LLM_DEFAULT_MAX_CONCURRENT_CALLS
            )
            or DEFAULTS.LLM_DEFAULT_MAX_CONCURRENT_CALLS
        ),
        llm_max_retry_attempts=int(
            data["llm"].get(
                "llm_max_retry_attempts", DEFAULTS.LLM_DEFAULT_MAX_RETRY_ATTEMPTS
            )
            or DEFAULTS.LLM_DEFAULT_MAX_RETRY_ATTEMPTS
        ),
        llm_max_input_tokens_per_call=data["llm"].get("llm_max_input_tokens_per_call"),
        llm_enable_token_rate_limiting=bool(
            data["llm"].get(
                "llm_enable_token_rate_limiting",
                DEFAULTS.LLM_ENABLE_TOKEN_RATE_LIMITING,
            )
        ),
        llm_max_input_tokens_per_minute=data["llm"].get(
            "llm_max_input_tokens_per_minute"
        ),
        enable_boundary_refinement=bool(
            data["llm"].get(
                "enable_boundary_refinement",
                DEFAULTS.ENABLE_BOUNDARY_REFINEMENT,
            )
        ),
        enable_word_level_boundary_refinder=bool(
            data["llm"].get(
                "enable_word_level_boundary_refinder",
                DEFAULTS.ENABLE_WORD_LEVEL_BOUNDARY_REFINDER,
            )
        ),
        output=data["output"],
        processing=data["processing"],
        background_update_interval_minute=data["app"].get(
            "background_update_interval_minute"
        ),
        post_cleanup_retention_days=data["app"].get("post_cleanup_retention_days"),
        whisper=whisper_obj,
        automatically_whitelist_new_episodes=bool(
            data["app"].get(
                "automatically_whitelist_new_episodes",
                DEFAULTS.APP_AUTOMATICALLY_WHITELIST_NEW_EPISODES,
            )
        ),
        number_of_episodes_to_whitelist_from_archive_of_new_feed=int(
            data["app"].get(
                "number_of_episodes_to_whitelist_from_archive_of_new_feed",
                DEFAULTS.APP_NUM_EPISODES_TO_WHITELIST_FROM_ARCHIVE_OF_NEW_FEED,
            )
            or DEFAULTS.APP_NUM_EPISODES_TO_WHITELIST_FROM_ARCHIVE_OF_NEW_FEED
        ),
        enable_public_landing_page=bool(
            data["app"].get(
                "enable_public_landing_page",
                DEFAULTS.APP_ENABLE_PUBLIC_LANDING_PAGE,
            )
        ),
        user_limit_total=data["app"].get(
            "user_limit_total", DEFAULTS.APP_USER_LIMIT_TOTAL
        ),
        autoprocess_on_download=bool(
            data["app"].get(
                "autoprocess_on_download",
                DEFAULTS.APP_AUTOPROCESS_ON_DOWNLOAD,
            )
        ),
        db_backup_enabled=bool(
            data["app"].get(
                "db_backup_enabled",
                DEFAULTS.APP_DB_BACKUP_ENABLED,
            )
        ),
        db_backup_interval_hours=int(
            data["app"].get(
                "db_backup_interval_hours",
                DEFAULTS.APP_DB_BACKUP_INTERVAL_HOURS,
            )
            or DEFAULTS.APP_DB_BACKUP_INTERVAL_HOURS
        ),
        db_backup_retention_count=int(
            data["app"].get(
                "db_backup_retention_count",
                DEFAULTS.APP_DB_BACKUP_RETENTION_COUNT,
            )
            or DEFAULTS.APP_DB_BACKUP_RETENTION_COUNT
        ),
    )


def hydrate_runtime_config_inplace(db_config: PydanticConfig | None = None) -> None:
    """Hydrate the in-process runtime config from DB-backed settings in-place.

    Preserves the identity of the `app.config` Pydantic instance so any modules
    that imported it by value continue to see updated fields.
    """
    cfg = db_config or to_pydantic_config()

    _log_initial_snapshot(cfg)

    _apply_top_level_env_overrides(cfg)

    _apply_whisper_env_overrides(cfg)

    _apply_llm_model_override(cfg)

    _apply_whisper_type_override(cfg)

    _commit_runtime_config(cfg)
    _log_final_snapshot()


def _log_initial_snapshot(cfg: PydanticConfig) -> None:
    logger.info(
        "Config hydration: starting with DB values | whisper_type=%s llm_model=%s openai_base_url=%s llm_api_key_set=%s whisper_api_key_set=%s",
        getattr(getattr(cfg, "whisper", None), "whisper_type", None),
        getattr(cfg, "llm_model", None),
        getattr(cfg, "openai_base_url", None),
        bool(getattr(cfg, "llm_api_key", None)),
        bool(getattr(getattr(cfg, "whisper", None), "api_key", None)),
    )


def _apply_top_level_env_overrides(cfg: PydanticConfig) -> None:
    env_llm_key = os.environ.get("LLM_API_KEY") or os.environ.get("OPENAI_API_KEY")
    if env_llm_key:
        cfg.llm_api_key = env_llm_key

    env_openai_base_url = os.environ.get("OPENAI_BASE_URL")
    if env_openai_base_url:
        cfg.openai_base_url = env_openai_base_url


def _apply_whisper_env_overrides(cfg: PydanticConfig) -> None:
    if cfg.whisper is None:
        return
    wtype = getattr(cfg.whisper, "whisper_type", None)
    if wtype == "remote":
        remote_key = (
            os.environ.get("WHISPER_API_KEY")
            or os.environ.get("WHISPER_REMOTE_API_KEY")
            or os.environ.get("OPENAI_API_KEY")
        )
        remote_base = (
            os.environ.get("WHISPER_BASE_URL")
            or os.environ.get("WHISPER_REMOTE_BASE_URL")
            or os.environ.get("OPENAI_BASE_URL")
        )
        remote_model = os.environ.get("WHISPER_MODEL") or os.environ.get(
            "WHISPER_REMOTE_MODEL"
        )
        if isinstance(cfg.whisper, RemoteWhisperConfig):
            if remote_key:
                cfg.whisper.api_key = remote_key
            if remote_base:
                cfg.whisper.base_url = remote_base
            if remote_model:
                cfg.whisper.model = remote_model


def _apply_llm_model_override(cfg: PydanticConfig) -> None:
    env_llm_model = os.environ.get("LLM_MODEL")
    if env_llm_model:
        cfg.llm_model = env_llm_model


def _configure_remote_whisper(cfg: PydanticConfig) -> None:
    """Configure remote whisper type."""
    existing_model_any = getattr(cfg.whisper, "model", "whisper-1")
    existing_model = (
        existing_model_any if isinstance(existing_model_any, str) else "whisper-1"
    )
    rem_model_env = os.environ.get("WHISPER_MODEL") or os.environ.get(
        "WHISPER_REMOTE_MODEL"
    )
    rem_model: str = (
        rem_model_env
        if isinstance(rem_model_env, str) and rem_model_env
        else existing_model
    )

    existing_key_any = getattr(cfg.whisper, "api_key", "")
    existing_key = existing_key_any if isinstance(existing_key_any, str) else ""
    rem_api_key_env = (
        os.environ.get("WHISPER_API_KEY")
        or os.environ.get("WHISPER_REMOTE_API_KEY")
        or os.environ.get("OPENAI_API_KEY")
    )
    rem_api_key: str = (
        rem_api_key_env
        if isinstance(rem_api_key_env, str) and rem_api_key_env
        else existing_key
    )

    existing_base_any = getattr(cfg.whisper, "base_url", "https://api.openai.com/v1")
    existing_base = (
        existing_base_any
        if isinstance(existing_base_any, str)
        else "https://api.openai.com/v1"
    )
    rem_base_env = (
        os.environ.get("WHISPER_BASE_URL")
        or os.environ.get("WHISPER_REMOTE_BASE_URL")
        or os.environ.get("OPENAI_BASE_URL")
    )
    rem_base_url: str = (
        rem_base_env
        if isinstance(rem_base_env, str) and rem_base_env
        else existing_base
    )

    existing_lang_any = getattr(cfg.whisper, "language", "en")
    lang: str = existing_lang_any if isinstance(existing_lang_any, str) else "en"

    timeout_sec: int = int(
        os.environ.get("WHISPER_TIMEOUT_SEC")
        or os.environ.get("WHISPER_REMOTE_TIMEOUT_SEC")
        or str(getattr(cfg.whisper, "timeout_sec", 600))
    )
    chunksize_mb: int = int(
        os.environ.get("WHISPER_CHUNKSIZE_MB")
        or os.environ.get("WHISPER_REMOTE_CHUNKSIZE_MB")
        or str(getattr(cfg.whisper, "chunksize_mb", 24))
    )

    cfg.whisper = RemoteWhisperConfig(
        model=rem_model,
        api_key=rem_api_key,
        base_url=rem_base_url,
        language=lang,
        timeout_sec=timeout_sec,
        chunksize_mb=chunksize_mb,
    )


def _apply_whisper_type_override(cfg: PydanticConfig) -> None:
    env_whisper_type = os.environ.get("WHISPER_TYPE")

    # Auto-detect whisper type from API key environment variables if not explicitly set
    if not env_whisper_type:
        if os.environ.get("WHISPER_API_KEY") or os.environ.get(
            "WHISPER_REMOTE_API_KEY"
        ):
            env_whisper_type = "remote"
            logger.info(
                "Auto-detected WHISPER_TYPE=remote from WHISPER_API_KEY / WHISPER_REMOTE_API_KEY environment variable"
            )

    if not env_whisper_type:
        return

    wtype = env_whisper_type.strip().lower()
    if wtype == "remote":
        _configure_remote_whisper(cfg)
    elif wtype == "test":
        cfg.whisper = TestWhisperConfig()


def _commit_runtime_config(cfg: PydanticConfig) -> None:
    logger.info(
        "Config hydration: after env overrides | whisper_type=%s llm_model=%s openai_base_url=%s llm_api_key_set=%s whisper_api_key_set=%s",
        getattr(getattr(cfg, "whisper", None), "whisper_type", None),
        getattr(cfg, "llm_model", None),
        getattr(cfg, "openai_base_url", None),
        bool(getattr(cfg, "llm_api_key", None)),
        bool(getattr(getattr(cfg, "whisper", None), "api_key", None)),
    )
    # Copy values from cfg to runtime_config, preserving Pydantic model instances
    for key in cfg.model_fields.keys():
        setattr(runtime_config, key, getattr(cfg, key))


def _log_final_snapshot() -> None:
    logger.info(
        "Config hydration: runtime set | whisper_type=%s llm_model=%s openai_base_url=%s",
        getattr(getattr(runtime_config, "whisper", None), "whisper_type", None),
        getattr(runtime_config, "llm_model", None),
        getattr(runtime_config, "openai_base_url", None),
    )


def ensure_defaults_and_hydrate() -> None:
    """Ensure default rows exist, then hydrate the runtime config from DB.

    Environment variable overrides are applied at read time (in read_combined
    and hydrate_runtime_config_inplace) — they are never written to the DB.
    """
    ensure_defaults()
    hydrate_runtime_config_inplace()
